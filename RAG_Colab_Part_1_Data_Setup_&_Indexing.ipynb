{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Part 1\n",
        "This section covers everything you need to set up your environment, process your PDF files, and load them into a Chroma vector database"
      ],
      "metadata": {
        "id": "snQSJ8GRAQMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate bitsandbytes -q\n",
        "!pip install -U pypdf -q\n",
        "!pip install -U chromadb -q\n",
        "!pip install -U sentence-transformers -q\n",
        "!pip install -U langchain -q\n",
        "\n",
        "import os\n",
        "import chromadb\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "xBHhPirPF3D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 1: SETUP, PDF PROCESSING, AND VECTOR DATABASE POPULATION\n",
        "# ==============================================================================\n",
        "# This script follows Phases 1 and 2 of the project roadmap.\n",
        "# It handles:\n",
        "# 1. Installation of required libraries.\n",
        "# 2. Setting up a directory for your PDF files.\n",
        "# 3. Processing the PDFs: loading, chunking text.\n",
        "# 4. Initializing ChromaDB.\n",
        "# 5. Generating vector embeddings and storing them in ChromaDB.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1.2 Set Up Data Directory ---\n",
        "# Creates a directory to store PDF files\n",
        "PDF_DIRECTORY = \"/content/hdm_pdfs/\"\n",
        "if not os.path.exists(PDF_DIRECTORY):\n",
        "    os.makedirs(PDF_DIRECTORY)\n",
        "    print(f\"Directory created: {PDF_DIRECTORY}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {PDF_DIRECTORY}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "xIUO4qgE7NjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2.2 Process PDFs ---\n",
        "def process_all_pdfs(directory_path):\n",
        "    \"\"\"\n",
        "    Loads all PDF files from a directory, extracts text, and splits it into chunks.\n",
        "\n",
        "    Args:\n",
        "        directory_path (str): The path to the directory containing PDF files.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text chunks (documents).\n",
        "        list: A list of corresponding metadata for each chunk.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "    all_metadata = []\n",
        "\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=150   # The number of characters to overlap between chunks\n",
        "    )\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(directory_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in {directory_path}.\")\n",
        "        return [], []\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF(s).\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        try:\n",
        "            file_path = os.path.join(directory_path, pdf_file)\n",
        "\n",
        "            # Read the PDF\n",
        "            reader = PdfReader(file_path)\n",
        "            pdf_text = \"\"\n",
        "            for page in reader.pages:\n",
        "                pdf_text += page.extract_text() or \"\"\n",
        "\n",
        "            # Split the text into chunks\n",
        "            chunks = text_splitter.split_text(pdf_text)\n",
        "\n",
        "            # Create metadata for each chunk\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                all_chunks.append(chunk)\n",
        "                all_metadata.append({'source': pdf_file, 'chunk_id': i})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_file}: {e}\")\n",
        "\n",
        "    print(f\"\\n PDF processing complete. Total chunks created: {len(all_chunks)}\")\n",
        "    return all_chunks, all_metadata\n",
        "\n",
        "# NOTE: Make sure you have uploaded your PDFs before running this cell.\n",
        "documents, metadatas = process_all_pdfs(PDF_DIRECTORY)\n"
      ],
      "metadata": {
        "id": "BtrbVcUzAxW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2.1 & 2.4 Set Up ChromaDB and Index Data ---\n",
        "def setup_and_populate_chromadb(docs, metadata_list):\n",
        "    \"\"\"\n",
        "    Initializes ChromaDB, generates embeddings for documents, and populates the database.\n",
        "\n",
        "    Args:\n",
        "        docs (list): The list of text chunks.\n",
        "        metadata_list (list): The list of metadata corresponding to the chunks.\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        print(\"No documents to process for ChromaDB.\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n--- Setting up ChromaDB and Populating ---\")\n",
        "\n",
        "    # Initialize ChromaDB client. Using a persistent client to save data in the Colab environment.\n",
        "    client = chromadb.PersistentClient(path=\"/content/chroma_db\")\n",
        "\n",
        "    # --- 2.3 Generate Embeddings ---\n",
        "    print(\"Loading sentence-transformer model 'all-MiniLM-L6-v2'...\")\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
        "\n",
        "    # Create a ChromaDB collection\n",
        "    # Using get_or_create to avoid errors if the collection already exists\n",
        "    collection_name = \"hdm_collection\"\n",
        "    collection = client.get_or_create_collection(name=collection_name)\n",
        "\n",
        "    # --- 2.4 Index Data in Chroma ---\n",
        "    # We need to create unique IDs for each entry.\n",
        "    ids = [f\"doc_{i}\" for i in range(len(docs))]\n",
        "\n",
        "    collection.add(\n",
        "        embeddings=embeddings,\n",
        "        documents=docs,\n",
        "        metadatas=metadata_list,\n",
        "        ids=ids\n",
        "    )\n",
        "\n",
        "    # Verify the number of items in the collection\n",
        "    count = collection.count()\n",
        "    print(f\"\\n ChromaDB setup complete. Collection '{collection_name}' now contains {count} items.\")\n",
        "    return collection\n",
        "\n",
        "# Execute the ChromaDB setup and population\n",
        "if documents:\n",
        "    hdm_collection = setup_and_populate_chromadb(documents, metadatas)\n",
        "else:\n",
        "    print(\"\\nSkipping ChromaDB setup because no documents were processed.\")"
      ],
      "metadata": {
        "id": "1yqPhHllA9Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SECTION 2: LLM INTEGRATION AND RAG CORE\n",
        "\n",
        "This script follows Phase 3 of the project roadmap.\n",
        " It handles:\n",
        " 1. Installation of the 'transformers' library for the LLM.\n",
        " 2. Loading the Qwen-3 model and its tokenizer.\n",
        " 3. Building the core RAG logic to query the database and generate answers."
      ],
      "metadata": {
        "id": "PAMo93HWaHTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1.1 Install Additional Libraries ---\n",
        "# We need the transformers library from Hugging Face to run the Qwen model,\n",
        "# as well as accelerate and bitsandbytes for memory optimization.\n",
        "#!pip install -U transformers accelerate -q\n",
        "#pip install -U bitsandbytes # Added this line to ensure the latest version is installed\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# --- 3.1 Set Up Qwen-3 LLM ---\n",
        "# We will load the model and tokenizer from Hugging Face.\n",
        "\n",
        "model_name = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define the quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# device_map=\"auto\" will automatically use the GPU if available.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config # Pass the config object here\n",
        ")\n",
        "print(\"model loaded successfully.\")"
      ],
      "metadata": {
        "id": "-FMwWTpyaR4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Re-initialize necessary components from Part 1 ---\n",
        "# This ensures that if you run this cell independently, it can reconnect\n",
        "# to the database and reload the embedding model.\n",
        "\n",
        "try:\n",
        "    # Connect to the existing ChromaDB client\n",
        "    client = chromadb.PersistentClient(path=\"/content/chroma_db\")\n",
        "    hdm_collection = client.get_collection(name=\"hdm_collection\")\n",
        "\n",
        "    # Load the embedding model\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error re-initializing: {e}\")\n",
        "    print(\"ensure you have run Part 1 successfully before this cell.\")"
      ],
      "metadata": {
        "id": "kDWJG-86ahit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.2 Build the RAG Pipeline ---\n",
        "def ask_hdm_expert(query, k=5):\n",
        "    \"\"\"\n",
        "    Performs the full RAG pipeline: embeds query, retrieves context, and generates an answer.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's question about the HDM model.\n",
        "        k (int): The number of relevant chunks to retrieve from the database.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer from the LLM.\n",
        "    \"\"\"\n",
        "    if 'hdm_collection' not in globals() or 'embedding_model' not in globals():\n",
        "        return \"Error: ChromaDB collection or embedding model not initialized. Please run Part 1.\"\n",
        "\n",
        "    # 1. & 2. Embed the user's query\n",
        "    query_embedding = embedding_model.encode(query)\n",
        "\n",
        "    # 3. Similarity Search in ChromaDB\n",
        "    results = hdm_collection.query(\n",
        "        query_embeddings=[query_embedding.tolist()],\n",
        "        n_results=k,\n",
        "        include=['documents', 'metadatas'] # Ensure metadata is included\n",
        "    )\n",
        "    retrieved_docs = results['documents'][0]\n",
        "    retrieved_metadatas = results['metadatas'][0]\n",
        "\n",
        "    # Format the retrieved documents into a single context string\n",
        "    context = \"\\n\\n---\\n\\n\".join(retrieved_docs)\n",
        "\n",
        "    # 4. Context Augmentation: Create the prompt for the LLM\n",
        "\n",
        "    # MAKE SURE TO CHANGE TEMPLATE BACK TO HDM CONTEXT!!!                                   <-----------------------------------------\n",
        "    prompt_template = f\"\"\"\n",
        "You are an expert research assistant specializing in american football.\n",
        "Answer the following question based *only* on the provided context in the vector database.\n",
        "If the context provided does not explicitly contain the answer, state that you cannot answer based on the provided information.\n",
        "\n",
        "**Context:**\n",
        "{context}\n",
        "\n",
        "**Question:**\n",
        "{query}\n",
        "\n",
        "**Answer:**\n",
        "\"\"\"\n",
        "\n",
        "    # 5. LLM Generation\n",
        "    print(\"Generating an answer...\")\n",
        "\n",
        "    # Use the chat template for better performance with chat-tuned models\n",
        "    # REMEMBER TO CHANGE BACK TO HDM CONTEXT!!!!!                                          <----------------------------------------------\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful expert on american football.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_template}\n",
        "    ]\n",
        "\n",
        "    # Set the pad_token to the eos_token if it's not already set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # FIX: create the formatted prompt string from the chat template.\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    # FIX: tokenize the formatted string to get the dictionary of inputs.\n",
        "    model_inputs = tokenizer(\n",
        "        formatted_prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Now model_inputs is a dictionary and can be correctly unpacked with **\n",
        "    outputs = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "    # Adjust the decoding to correctly slice the output tensor\n",
        "    response_text = tokenizer.decode(outputs[0][model_inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    return {\"answer\": response_text, \"sources\": retrieved_metadatas}\n"
      ],
      "metadata": {
        "id": "CXgE2s5jajBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example Usage ---\n",
        "example_query = \"What are the different ways i can score points in football?\"\n",
        "result = ask_hdm_expert(example_query)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Query: {example_query}\")\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(f\"Answer: {result['answer']}\")\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"Sources Used by the Model:\")\n",
        "for i, source in enumerate(result['sources']):\n",
        "    print(f\"  Source {i+1}: From '{source['source']}' (Chunk ID: {source['chunk_id']})\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "VB-eH8SQa2Pc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}