{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM3oXM6z3uhSigC4FXqigrH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qaanit/Project-Halyard/blob/main/HDM_LLM_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Loading model and tokenizer**\n",
        "\n",
        "Make sure you are connected to T4 or better hosted runtime as Unsloth only works on NVIDIA GPUs Intel and CPUs.\n",
        "\n",
        "If only connecting now, rerun everything before this as well."
      ],
      "metadata": {
        "id": "ay2PW81bEwWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwfVoPiQ_dD4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One must patch the DPO Trainer first!\n",
        "from unsloth import PatchDPOTrainer\n",
        "\n",
        "PatchDPOTrainer()"
      ],
      "metadata": {
        "id": "np1pkecItKfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
        "\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Phi-4\",\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\",\n",
        "    max_seq_length = 1024,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")\n",
        "\n",
        "#model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#    model_name = \"unsloth/zephyr-sft-bnb-4bit\",\n",
        "#    max_seq_length = 2048,\n",
        "#    dtype = None,\n",
        "#    load_in_4bit = True,\n",
        "#)"
      ],
      "metadata": {
        "id": "1kIzA7TzEzxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Uploading the training dataset\n"
      ],
      "metadata": {
        "id": "t8rzjlmcPZzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: upload the unsloth hdm ai training dataset.json as the training dataset for the model\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/Corrected UnSloth HDM AI Training Dataset.json\", encoding=\"utf-8\", split = \"train\")"
      ],
      "metadata": {
        "id": "7ZntsJrU46Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split it into 90% for training and 10% for evaluation\n",
        "dataset_split = dataset.train_test_split(test_size=0.25, seed=42)\n",
        "\n",
        "train_dataset = dataset_split[\"train\"]\n",
        "eval_dataset = dataset_split[\"test\"] # This is your evaluation set\n",
        "\n",
        "print(f\"Training on {len(train_dataset)} examples.\")\n",
        "print(f\"Evaluating on {len(eval_dataset)} examples.\")\n"
      ],
      "metadata": {
        "id": "UF8VtN0HPvkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the LoRa hyperparameters\n"
      ],
      "metadata": {
        "id": "jQaZU_7neAPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,   # We support rank stabilized LoRA\n",
        "    loftq_config = None,  # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "wB2-p9lFZ-_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the padding token if it's not already defined\n",
        "#if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "CF6ryvhyWcsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Converting the data into conversational format\n",
        "The dataset need not be changed to a converstional format as the format is already structured for **Direct Preference Optimization (DPO)**. This is a much more advanced technique compared to standard fine-tuning as it trains the model to understand *why* the chosen response is better than the rejected.\n",
        "\n",
        "Training arguments:\n",
        "\n",
        "\n",
        "*   **per_device_train_batch_size**: The number of training examples processed at once on a single GPU. A higher value uses more VRAM.\n",
        "*   **gradient_accumulation_steps**: Simulates a larger batch size without using more VRAM. It processes N small batches and only updates the model weights after all N have been seen.\n",
        "*   **num_train_epochs**: The total number of times the trainer will iterate over the entire dataset.\n",
        "*   **max_steps**: An alternative to num_train_epochs. It specifies the exact number of model updates to perform.\n",
        "* **warmup_ratio**: The portion of training where the learning rate gradually increases from 0 to its target value. This prevents the model from being destabilized by large weight updates at the very beginning.\n",
        "* **learning_rate**: The step size for each model update. Too high, and the training becomes unstable; too low, and it learns too slowly.\n",
        "* **lr_scheduler_type**: Determines the pattern for how the learning rate changes after the warmup phase. Linear is default.\n",
        "* **optim**: The optimization algorithm used to update the model's weights. \"adamw_8bit\" is the best choice for Unsloth on Colab."
      ],
      "metadata": {
        "id": "Kp1AUG-IRMhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, PatchDPOTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "PatchDPOTrainer()\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model = model,\n",
        "    ref_model = None, # Unsloth handles this automatically\n",
        "    args = DPOConfig(\n",
        "        per_device_train_batch_size = 1, # Use 1 for larger models on free Colab\n",
        "        gradient_accumulation_steps = 4, # Simulates a larger batch size\n",
        "        warmup_ratio = 0.1,\n",
        "        max_steps = 10,\n",
        "        num_train_epochs = 1, # A single epoch is often enough for DPO\n",
        "        learning_rate = 5e-6, # A lower learning rate is recommended for DPO\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 2,\n",
        "        optim = \"adamw_8bit\",\n",
        "        seed = 42,\n",
        "        output_dir = \"outputs_dpo\",\n",
        "\n",
        "        eval_strategy = \"steps\",      # Evaluate at regular step intervals\n",
        "        eval_steps = 2,                    # How often to run evaluation (e.g., every 20 steps)\n",
        "        load_best_model_at_end = True,      # Automatically load the best model (lowest eval_loss) at the end\n",
        "        save_total_limit = 2,\n",
        "    ),\n",
        "    beta = 0.05,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    max_length = 4096,          # Max length of prompt + response\n",
        "    max_prompt_length = 2048,   # Max length of the prompt section\n",
        ")"
      ],
      "metadata": {
        "id": "9Vttm3VMRLnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the model - Understanding the DPO log\n",
        "\n",
        "\n",
        "\n",
        "*   **Step**: The training update number. Each step represents one update to the model's weights.\n",
        "\n",
        "*   **Training Loss**: The most important metric. This is the DPO loss. It measures how well the model is learning to prefer the chosen response over the rejected one.\n",
        "\n",
        "* **rewards / chosen**: The reward score calculated for the chosen (good) response. Higher is better.\n",
        "\n",
        "* **rewards / rejected**: The reward score for the rejected (bad) response. Lower is better.\n",
        "\n",
        "* **rewards / accuracies**: The accuracy of the model in correctly giving the chosen response a higher reward than the rejected one for a given batch.\n",
        "\n",
        "* **rewards / margins**: The difference between the chosen and rejected rewards (chosen - rejected). A larger positive margin is better."
      ],
      "metadata": {
        "id": "nsJoWE_bSrNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_trainer.train()"
      ],
      "metadata": {
        "id": "y5oNscC2GQ-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "# --- After dpo_trainer.train() has completed ---\n",
        "\n",
        "# 1. Optimize model for inference\n",
        "# This enables Unsloth's native 2x faster inference for generation\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# 2. Prepare your input prompt using the chat template\n",
        "# The tokenizer automatically knows the correct chat format for the Qwen3 model.\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"?\"},\n",
        "]\n",
        "\n",
        "# Apply the chat template to format the messages into a single string\n",
        "# `tokenize=False` returns the string, `add_generation_prompt=True` adds the final assistant prompt part\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking = True)\n",
        "\n",
        "print(\"--- Your Input Prompt ---\")\n",
        "print(prompt)\n",
        "\n",
        "# 3. Tokenize the input and move to GPU\n",
        "inputs = tokenizer(\n",
        "    [prompt],\n",
        "    return_tensors = \"pt\"\n",
        ").to(\"cuda\") # Ensure tensors are on the GPU\n",
        "\n",
        "# 4. Generate a response using TextStreamer for streaming output\n",
        "print(\"\\n--- Model's Response (Streaming) ---\")\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 1024, # Adjust the maximum number of tokens to generate\n",
        "    use_cache = True,\n",
        "    pad_token_id = tokenizer.eos_token_id,\n",
        "    eos_token_id = tokenizer.eos_token_id,\n",
        "    # Optional generation parameters for more creative/diverse outputs:\n",
        "    # do_sample=True,\n",
        "    # temperature=0.7,\n",
        "    # top_p=0.9,\n",
        "    # repetition_penalty=1.1,\n",
        ")"
      ],
      "metadata": {
        "id": "h28q4rZZB1Ao"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}